{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Regex Based Text Tokenization"
      ],
      "metadata": {
        "id": "iICcbV7C7uIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpMtdD4l7R3f",
        "outputId": "9b2e68af-1197-4a1c-96fd-73b430ffd8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'a', 'diplomatic', 'cable', 'sent', 'July', '8', ',', 'embassy', 'Charge', 'd’Affairs', 'David', 'Greene', 'asked', 'whether', 'the', 'embassy', 'could', 'process', 'claims', 'from', 'other', 'minority', 'groups', 'claiming', 'race-based', 'discrimination', 'such', 'as', '\"', 'coloured', '\"', 'South', 'Africans', 'who', 'speak', 'Afrikaans', '.', 'In', 'South', 'Africa', 'the', 'term', 'coloured', 'refers', 'to', 'mixed-raced', 'people', ',', 'a', 'classification', 'created', 'by', 'the', 'apartheid', 'regime', 'still', 'in', 'use', 'today', '.', 'The', 'answer', 'came', 'back', 'days', 'later', 'in', 'an', 'email', 'from', 'Spencer', 'Chretien', ',', 'the', 'highest-ranking', 'official', 'in', 'the', 'State', \"Department's\", 'refugee', 'and', 'migration', 'bureau', ',', 'saying', 'the', 'program', 'is', 'intended', 'for', 'white', 'people', '.', 'Reuters', 'was', 'unable', 'to', 'independently', 'verify', 'the', 'precise', 'language', 'in', 'the', 'email', 'which', 'was', 'described', 'to', 'the', 'news', 'agency', 'by', 'three', 'sources', 'familiar', 'with', 'its', 'contents', '.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "with open('data/ch2_pset_text.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "#reprocessed = re.split(r'([,.:;?_!\"()\\s]|--)', text)\n",
        "process_text = re.split(r'([,.:;?_!\"()\\s]|--...)', raw_text)\n",
        "process_text = [item.strip() for item in process_text if item.strip()]\n",
        "print(process_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Build Tokenizer Class"
      ],
      "metadata": {
        "id": "I6mmz-tf-ysa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make vocabulary"
      ],
      "metadata": {
        "id": "63RgeFzUFz5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create vocab\n",
        "\n",
        "alphabetical_text = sorted(set(process_text))\n",
        "\n",
        "#add weird chars\n",
        "alphabetical_text.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocabulary = {word : index for index, word in enumerate(alphabetical_text)}\n",
        "\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKh5Eb2i9mn2",
        "outputId": "06cc3869-d30f-43b3-dc09-f50d32360d93"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, ',': 1, '.': 2, '8': 3, 'Africa': 4, 'Africans': 5, 'Afrikaans': 6, 'Charge': 7, 'Chretien': 8, 'David': 9, \"Department's\": 10, 'Greene': 11, 'In': 12, 'July': 13, 'Reuters': 14, 'South': 15, 'Spencer': 16, 'State': 17, 'The': 18, 'a': 19, 'agency': 20, 'an': 21, 'and': 22, 'answer': 23, 'apartheid': 24, 'as': 25, 'asked': 26, 'back': 27, 'bureau': 28, 'by': 29, 'cable': 30, 'came': 31, 'claiming': 32, 'claims': 33, 'classification': 34, 'coloured': 35, 'contents': 36, 'could': 37, 'created': 38, 'days': 39, 'described': 40, 'diplomatic': 41, 'discrimination': 42, 'd’Affairs': 43, 'email': 44, 'embassy': 45, 'familiar': 46, 'for': 47, 'from': 48, 'groups': 49, 'highest-ranking': 50, 'in': 51, 'independently': 52, 'intended': 53, 'is': 54, 'its': 55, 'language': 56, 'later': 57, 'migration': 58, 'minority': 59, 'mixed-raced': 60, 'news': 61, 'official': 62, 'other': 63, 'people': 64, 'precise': 65, 'process': 66, 'program': 67, 'race-based': 68, 'refers': 69, 'refugee': 70, 'regime': 71, 'saying': 72, 'sent': 73, 'sources': 74, 'speak': 75, 'still': 76, 'such': 77, 'term': 78, 'the': 79, 'three': 80, 'to': 81, 'today': 82, 'unable': 83, 'use': 84, 'verify': 85, 'was': 86, 'whether': 87, 'which': 88, 'white': 89, 'who': 90, 'with': 91, '<|endoftext|>': 92, '<|unk|>': 93}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Class\n"
      ],
      "metadata": {
        "id": "w__94TjtF1dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, vocabulary):\n",
        "    self.word_to_token = vocabulary\n",
        "    self.token_to_word = {vocabulary[key] : key for key in vocabulary}\n",
        "\n",
        "  def encode(self, text) -> list[int]:\n",
        "    preprocess = re.split(r'([,.:;?_!\"()\\s]|--...)', text)\n",
        "    preprocess = [item.strip() for item in preprocess if item.strip()]\n",
        "    return [self.word_to_token[word] for word in preprocess]\n",
        "\n",
        "  def decode(self, encoded:list[int]) -> list[str]:\n",
        "    return [self.token_to_word[token] for token in encoded]\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(vocabulary)\n",
        "print(tokenizer.encode(raw_text))\n",
        "print(tokenizer.decode(tokenizer.encode(raw_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM0VNBGjE56u",
        "outputId": "75329aa7-3367-4343-bd25-39d7c8ebd971"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12, 19, 41, 30, 73, 13, 3, 1, 45, 7, 43, 9, 11, 26, 87, 79, 45, 37, 66, 33, 48, 63, 59, 49, 32, 68, 42, 77, 25, 0, 35, 0, 15, 5, 90, 75, 6, 2, 12, 15, 4, 79, 78, 35, 69, 81, 60, 64, 1, 19, 34, 38, 29, 79, 24, 71, 76, 51, 84, 82, 2, 18, 23, 31, 27, 39, 57, 51, 21, 44, 48, 16, 8, 1, 79, 50, 62, 51, 79, 17, 10, 70, 22, 58, 28, 1, 72, 79, 67, 54, 53, 47, 89, 64, 2, 14, 86, 83, 81, 52, 85, 79, 65, 56, 51, 79, 44, 88, 86, 40, 81, 79, 61, 20, 29, 80, 74, 46, 91, 55, 36, 2]\n",
            "['In', 'a', 'diplomatic', 'cable', 'sent', 'July', '8', ',', 'embassy', 'Charge', 'd’Affairs', 'David', 'Greene', 'asked', 'whether', 'the', 'embassy', 'could', 'process', 'claims', 'from', 'other', 'minority', 'groups', 'claiming', 'race-based', 'discrimination', 'such', 'as', '\"', 'coloured', '\"', 'South', 'Africans', 'who', 'speak', 'Afrikaans', '.', 'In', 'South', 'Africa', 'the', 'term', 'coloured', 'refers', 'to', 'mixed-raced', 'people', ',', 'a', 'classification', 'created', 'by', 'the', 'apartheid', 'regime', 'still', 'in', 'use', 'today', '.', 'The', 'answer', 'came', 'back', 'days', 'later', 'in', 'an', 'email', 'from', 'Spencer', 'Chretien', ',', 'the', 'highest-ranking', 'official', 'in', 'the', 'State', \"Department's\", 'refugee', 'and', 'migration', 'bureau', ',', 'saying', 'the', 'program', 'is', 'intended', 'for', 'white', 'people', '.', 'Reuters', 'was', 'unable', 'to', 'independently', 'verify', 'the', 'precise', 'language', 'in', 'the', 'email', 'which', 'was', 'described', 'to', 'the', 'news', 'agency', 'by', 'three', 'sources', 'familiar', 'with', 'its', 'contents', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4: Creating Training Sequences with Dataset Class"
      ],
      "metadata": {
        "id": "18O9iXrxJm-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(self, txt, tokenizer, max_length, stride)\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input = []\n",
        "    self.label = []\n",
        "\n",
        "    encoded_text = tokenizer.encode(raw_text)\n",
        "    assert len(encoded_text) > max_length\n",
        "\n",
        "    for i in range(0, len(encoded_text) - max_length, stride):\n",
        "      inputs = encoded_text[i : i + max_length]\n",
        "      labels = encoded_text[i + 1 : (i+1) + max_length]\n",
        "      self.input.append(torch.tensor(inputs))\n",
        "      self.label.append(torch.tensor(labels))\n",
        "\n",
        "    def get_length(self) -> int:\n",
        "      return len(self.input)\n",
        "\n",
        "    def get_item(self, index):\n",
        "      return (self.input[index], self.label[index])\n",
        "\n"
      ],
      "metadata": {
        "id": "XHKLgbyfGxEB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ws-xrll-OF7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}