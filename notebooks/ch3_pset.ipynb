{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: Implement a Self Attention Step for the Given Vector\n",
        "```python\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # Your   (x^1)\n",
        "     [0.55, 0.87, 0.66],  # journey (x^2)\n",
        "     [0.57, 0.85, 0.64],  # starts (x^3)\n",
        "     [0.22, 0.58, 0.33],  # with   (x^4)\n",
        "     [0.77, 0.25, 0.10],  # one    (x^5)\n",
        "     [0.05, 0.80, 0.55]], # step   (x^6)\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "Expected output\n",
        "`Context vector for 'journey': tensor([0.4419, 0.6515, 0.5683])`"
      ],
      "metadata": {
        "id": "lxQhtm_Myr9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # Your   (x^1)\n",
        "     [0.55, 0.87, 0.66],  # journey (x^2)\n",
        "     [0.57, 0.85, 0.64],  # starts (x^3)\n",
        "     [0.22, 0.58, 0.33],  # with   (x^4)\n",
        "     [0.77, 0.25, 0.10],  # one    (x^5)\n",
        "     [0.05, 0.80, 0.55]], # step   (x^6)\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "x_2 = inputs[1]\n",
        "unnormalized_attn_scores = x_2 @ (inputs).transpose(0,1)\n",
        "normalized_attn_scores = torch.softmax(unnormalized_attn_scores, dim=0) #column)\n",
        "context_vector = normalized_attn_scores @ inputs\n",
        "print(\"Context for journey: \", context_vector)\n",
        "\n",
        "#even though mathematically represented differently, operations for keys and queries are all very similiar."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV-YT8G1ykIS",
        "outputId": "106f4d78-7714-4a24-a8bb-30490eaabbb2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context for journey:  tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention with Trainable Projections\n",
        "\n",
        "Given the input tensor, return the all context vectors\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```python\n",
        "tensor([[0.2996, 0.8053],\n",
        "[0.3061, 0.8210],\n",
        "[0.3058, 0.8203],\n",
        "[0.2948, 0.7939],\n",
        "[0.2927, 0.7891],\n",
        "[0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
        "```"
      ],
      "metadata": {
        "id": "HlBoWczr3-QD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EoBlk0ityNmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ade6d6e-b466-4941-b642-26bc817288d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>) torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # Your   (x^1)\n",
        "     [0.55, 0.87, 0.66],  # journey (x^2)\n",
        "     [0.57, 0.85, 0.64],  # starts (x^3)\n",
        "     [0.22, 0.58, 0.33],  # with   (x^4)\n",
        "     [0.77, 0.25, 0.10],  # one    (x^5)\n",
        "     [0.05, 0.80, 0.55]], # step   (x^6)\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "#for randomness at some point\n",
        "torch.manual_seed(123)\n",
        "#d_in represents to how much elements per vector in inputs\n",
        "#d_out is design choice .putting in a pin, acccepting fo rnow\n",
        "d_in, d_out = inputs.shape[1], 2\n",
        "\n",
        "#initializing random weights\n",
        "W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "W_value = nn.Parameter(torch.rand(d_in,d_out))\n",
        "\n",
        "\n",
        "#calculating values\n",
        "queries = inputs @ W_query\n",
        "keys= inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "'''\n",
        "notice by this stage we have stuff in the space of the answer. but\n",
        "it's important to know, as a question im going to ask,\n",
        "why exactly the shape evolves the way it does.\n",
        "'''\n",
        "\n",
        "unnormalized_attn_scores = queries @ keys.transpose(0,1)\n",
        "normalized_attn_scores = torch.softmax((unnormalized_attn_scores) / math.sqrt(keys.shape[-1]), dim=-1) #dim -1 is applied whenever we are\n",
        "#calculating entire tensors of information. dim=0 was only because we were only dealing with 1\n",
        "context_vectors = normalized_attn_scores @ values\n",
        "print(context_vectors, context_vectors.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: Casual Attention with Masking Tokens\n",
        "\n",
        "Expected Output:\n",
        "```python\n",
        "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
        "[0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
        "[0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
        "[0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
        "[0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
        "[0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
        "grad_fn=<DivBackward0>)\n",
        "```"
      ],
      "metadata": {
        "id": "1TIWv6yS_Vsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = unnormalized_attn_scores.shape[0] #accept this to be true, it makes snse it has to be the same size anyway\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) #just creates an empty vector that is the mask\n",
        "masked = unnormalized_attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "normalized_attn_scores = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(normalized_attn_scores)\n",
        "\n",
        "\n",
        "#accept most of this as truth, it's how to implement it\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRrbI4vYAk8t",
        "outputId": "4cdf1dcd-15e4-4d36-878b-ec6b49108321"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
              "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
              "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports=========================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#imports=========================\n",
        "\n",
        "class CasualAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_values = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # also need to initialize mask and dropout\n",
        "        # Corrected: Initialize dropout as an nn.Dropout module\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "        self.context_length = context_length\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        queries, keys, values = self.W_query(x), self.W_keys(x), self.W_values(x)\n",
        "        unnormalized_attn_scores = queries @ (keys.transpose(1,2))\n",
        "\n",
        "        #mas/dropout phase\n",
        "        unnormalized_attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        normalized_attn_scores = torch.softmax(unnormalized_attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "        normalized_attn_scores = self.dropout(normalized_attn_scores)\n",
        "\n",
        "        #return context vector\n",
        "\n",
        "        return normalized_attn_scores @ values\n",
        "\n",
        "\n",
        "#=========================\n",
        "'''\n",
        "file: SelfAttentionv1.py\n",
        "purpose: Initiate a class to implement self attention mechanisms.\n",
        "'''\n",
        "#=========================\n",
        "\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        '''\n",
        "        nn.Linear asserts the weight values, but also can be used to calcluate the queries\n",
        "        values\n",
        "        '''\n",
        "    def forward(self, x):# where x is the tensor\n",
        "        queries = self.W_query(x)\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        unnormalized_attn_scores = queries @ keys.transpose(0,1)\n",
        "        normalized_attn_scores = torch.softmax((unnormalized_attn_scores) / (keys.shape[-1])**(.5), dim=-1)\n",
        "        context_vector = normalized_attn_scores @ values\n",
        "\n",
        "        return context_vector"
      ],
      "metadata": {
        "id": "0EGM2nBYBAfj"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # Your   (x^1)\n",
        "     [0.55, 0.87, 0.66],  # journey (x^2)\n",
        "     [0.57, 0.85, 0.64],  # starts (x^3)\n",
        "     [0.22, 0.58, 0.33],  # with   (x^4)\n",
        "     [0.77, 0.25, 0.10],  # one    (x^5)\n",
        "     [0.05, 0.80, 0.55]], # step   (x^6)\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "# Corrected: access the integer value from inputs[0].shape\n",
        "d_in, d_out, context_length = inputs[0].shape[0], 2, batch.shape[1]\n",
        "ca_mech = CasualAttention(d_in, d_out, context_length, 0.0)\n",
        "context_batch = ca_mech.forward(batch)\n",
        "print(context_batch)\n",
        "\n",
        "\n",
        "sa_mech = SelfAttention(d_in, d_out)\n",
        "context_tensor = sa_mech.forward(inputs)\n",
        "print(context_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd1uyDLVKVys",
        "outputId": "c970685a-d02d-406f-dbf1-f9b789e23a88"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0844,  0.0414],\n",
            "         [-0.2264, -0.0039],\n",
            "         [-0.4163, -0.0564],\n",
            "         [-0.5014, -0.1011],\n",
            "         [-0.7754, -0.1867],\n",
            "         [-1.1632, -0.3303]],\n",
            "\n",
            "        [[-0.0844,  0.0414],\n",
            "         [-0.2264, -0.0039],\n",
            "         [-0.4163, -0.0564],\n",
            "         [-0.5014, -0.1011],\n",
            "         [-0.7754, -0.1867],\n",
            "         [-1.1632, -0.3303]]], grad_fn=<UnsafeViewBackward0>)\n",
            "tensor([[0.5085, 0.3508],\n",
            "        [0.5084, 0.3508],\n",
            "        [0.5084, 0.3506],\n",
            "        [0.5074, 0.3471],\n",
            "        [0.5076, 0.3446],\n",
            "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}