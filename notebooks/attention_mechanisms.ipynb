{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## What is attention mechanism\n",
        "\n",
        "*   Words represented as vectors don't consider context (bat (baseball), bat (animal) are the same vector\n",
        "*  Attention mechanisms changes the original vector with maybe multiple meanings by considering it's context. SUPA interesting.\n",
        "* Even more, when the weights are applied, a vector for 'bat'  (the animal) can be shifted into a direction of maybe caves, blindness, etc..\n",
        "\n"
      ],
      "metadata": {
        "id": "5y_Y9UJaV3AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Omega -> Context Vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "Q_i = x_i W^Q, \\quad K_j = x_j W^K, \\quad V_j = x_j W^V\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "\n",
        "$$\n",
        " \\omega_{ij} = Q_i \\cdot K_j^T\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} = \\text{softmax}(\\omega_{ij})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "\n",
        "$$\n",
        "z_i = \\sum_{j=1}^{n} \\alpha_{ij} V_j\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "zzxMiHQJXgFl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC2OYBcCTyhE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{\"Your journey starts with one step\"}\n",
        "\\;\\longrightarrow\\;\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}^{(1)} &= \\begin{bmatrix} 0.43 & 0.15 & 0.89 \\end{bmatrix}\n",
        "  \\;\\bigl(\\text{“Your”}\\bigr)\\\\\n",
        "\\mathbf{x}^{(2)} &= \\begin{bmatrix} 0.55 & 0.87 & 0.66 \\end{bmatrix}\n",
        "  \\;\\bigl(\\text{“journey”}\\bigr)\\\\\n",
        "\\mathbf{x}^{(3)} &= \\begin{bmatrix} 0.57 & 0.85 & 0.64 \\end{bmatrix}\n",
        "  \\;\\bigl(\\text{“starts”}\\bigr)\\\\\n",
        "\\mathbf{x}^{(4)} &= \\begin{bmatrix} 0.22 & 0.58 & 0.33 \\end{bmatrix}\n",
        "  \\;\\bigl(\\text{“with”}\\bigr)\\\\\n",
        "\\mathbf{x}^{(5)} &= \\begin{bmatrix} 0.77 & 0.25 & 0.10 \\end{bmatrix}\n",
        "  \\;\\bigl(\\text{“one”}\\bigr)\\\\\n",
        "\\mathbf{x}^{(6)} &= \\begin{bmatrix} 0.05 & 0.80 & 0.55 \\end{bmatrix}\n",
        "  \\;\\bigl(\\text{“step”}\\bigr)\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "xn_-S_wESSCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Calculation of Attention Weights\n",
        "\n",
        "Magnitude of  $\n",
        "\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos(\\theta)\n",
        "$ determines how aligned two vectors are, thus a mathematical representation of context when words are represented in space.  After we calculate the dot product of the query vector (the current vector we are at) with the vectors of every other word in the context, we create this score\n",
        "\n",
        "$$\n",
        "\\vec{\\alpha_{2j}} =\n",
        "\\begin{bmatrix}\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.9544 \\\\\n",
        "1.4950 \\\\\n",
        "1.4754 \\\\\n",
        "0.8434 \\\\\n",
        "0.7070 \\\\\n",
        "1.0865\n",
        "\\end{bmatrix} = \\sum_{j=1}^{n} \\alpha_{2j}\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-rYHoWa9bD1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]\n",
        "\n",
        "\n",
        "print(inputs.shape[0])\n",
        "attention_score_x2 = torch.empty(inputs.shape[0])\n",
        "for (index, x_i) in enumerate(inputs):\n",
        "  attention_score_x2[index] = torch.dot(x_i, query)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS9rW5eZZBdi",
        "outputId": "b4a5d3a5-8d97-409b-9acb-43d1f5e76ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing\n",
        "\n",
        "These sum to one through a normailization process that, for lack of better words, yields 'meaningful' results. Mathematically, for later implementation. But in general just use the pytorch one -- it's robust.\n",
        "\n",
        "$$$$\n",
        "\n",
        "$$\n",
        "\\vec{z_2} =\n",
        "\\begin{bmatrix}\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.9544 \\\\\n",
        "1.4950 \\\\\n",
        "1.4754 \\\\\n",
        "0.8434 \\\\\n",
        "0.7070 \\\\\n",
        "1.0865\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\text{softmax}(x_i)\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.1385 \\\\\n",
        "0.2379 \\\\\n",
        "0.2333 \\\\\n",
        "0.1240 \\\\\n",
        "0.1082 \\\\\n",
        "0.1581\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "$$$$\n",
        "\n",
        "where\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "8OHo7FCZe8-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def softmax_naive(x):\n",
        " return torch.exp(x) / torch.exp(x).sum(dim=0) #dim=0 because we're summing a column.\n",
        "\n",
        "\n",
        "attn_weights_2_solid = torch.softmax(attention_score_x2, dim=0)\n",
        "print(\"Refined Attention Weights:\", attn_weights_2_solid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRqoiQ9dbXQI",
        "outputId": "594c4127-c968-41a8-eb10-9fc130f5d387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Attention Weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalizing the Calculation of $z$\n",
        "$$\n",
        "\\vec{\\alpha_{2j}} =\n",
        "\\begin{bmatrix}\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.9544 \\\\\n",
        "1.4950 \\\\\n",
        "1.4754 \\\\\n",
        "0.8434 \\\\\n",
        "0.7070 \\\\\n",
        "1.0865\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\text{softmax}(x_i)\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.1385 \\\\\n",
        "0.2379 \\\\\n",
        "0.2333 \\\\\n",
        "0.1240 \\\\\n",
        "0.1082 \\\\\n",
        "0.1581\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\vec{z_{2}} =\n",
        "\\begin{bmatrix}\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\alpha_{21} V_1 + \\alpha_{22} V_2 + \\cdots\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.4371 \\\\\n",
        "0.4371 \\\\\n",
        "0.4371 \\\\\n",
        "0.4371 \\\\\n",
        "0.4371 \\\\\n",
        "0.4371\n",
        "\\end{bmatrix} = z_2 =\\sum_j \\alpha_{2j}V_j\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "AlKfsVEgj95S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vector = torch.empty(inputs.shape[0])\n",
        "\n",
        "for (index, x_j) in enumerate(query):\n",
        "  context_vector += attn_weights_2_solid[index] * x_j\n",
        "\n",
        "print(context_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5czdQmfue-1k",
        "outputId": "b9574ce4-019e-4d1b-cce1-241cee7cd2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Attention Weights $\\rightarrow$ context vectors for all Input Tokens\n",
        "\n",
        "The latter example is a very quick way of calculating the normalized attentino score for the group of tensors.\n",
        "\n"
      ],
      "metadata": {
        "id": "nsQCSvQFjsnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "attn_scores = torch.empty(6,6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "\n",
        "attn_scores = inputs @ inputs.T\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)\n",
        "context_vector = attn_weights @ inputs\n",
        "print(context_vector)"
      ],
      "metadata": {
        "id": "embEVgvdkrQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a381a8-0039-4c3e-d800-5e8a45890611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A quick verification softmax was applied properly\n",
        "\n",
        "Especially in pertains to `dim=-1` which may change from code to code."
      ],
      "metadata": {
        "id": "4domJENEhlLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrNv-kHAe7eo",
        "outputId": "678750f4-ed6c-4bbe-e2d2-3140299a1c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weights Q, K, V\n",
        "\n",
        "\n",
        "We left these alone a while ago, now we are going back to compute them and find out their significance. Mathematically, this is what we are doing\n",
        "$$\n",
        "Q_i = x_i W^Q, \\quad K_j = x_j W^K, \\quad V_j = x_j W^V\n",
        "$$"
      ],
      "metadata": {
        "id": "JUC40g6DimFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2\n",
        "\n",
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io9BQEepho5z",
        "outputId": "9b81d58f-15e7-47ef-83f5-bf9fac7ec7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice mathematically that $Q_i$ only considers $i$, because it is the query vector and only considers itself. that means that we only need to perform the `query_2 = x_2 @ W_query` operation. However, because $(K,V)$ consider $j$ we need to perform these operations on all vectors, below:\n",
        "\n",
        "also, @. is just shorthand for a double for loop. whenever you feel like you need to do that, you know, use @."
      ],
      "metadata": {
        "id": "z4YHzbZhmEue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(keys, values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m9opA5jlpJo",
        "outputId": "680580a8-29e5-45d9-bdd9-068d340584fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3669, 0.7646],\n",
            "        [0.4433, 1.1419],\n",
            "        [0.4361, 1.1156],\n",
            "        [0.2408, 0.6706],\n",
            "        [0.1827, 0.3292],\n",
            "        [0.3275, 0.9642]]) tensor([[0.1855, 0.8812],\n",
            "        [0.3951, 1.0037],\n",
            "        [0.3879, 0.9831],\n",
            "        [0.2393, 0.5493],\n",
            "        [0.1492, 0.3346],\n",
            "        [0.3221, 0.7863]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recomputing Context Vectors with Calculated Weights\n",
        "\n",
        "starting with $$\n",
        " \\omega_{ij} = Q_i \\cdot K_j^T\n",
        "$$"
      ],
      "metadata": {
        "id": "XYsXR6AsmxJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "attn_scores_2 = query_2 @ keys.T\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWNwr8mSmcvo",
        "outputId": "81a49fb9-031a-4c6f-dd53-0ab600dd3fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$\n",
        "\\alpha_{ij} = \\text{softmax}(\\omega_{ij})\n",
        "$$"
      ],
      "metadata": {
        "id": "kcgUGTdgoCMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2, sum(attn_weights_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAf3FHHMnuHW",
        "outputId": "7084b758-37d1-49d2-c148-0aadf60e7f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820]) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$z_2 = \\sum_j \\alpha_{2j} V_j$$"
      ],
      "metadata": {
        "id": "qBsh25cvqbsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vector_2 = attn_weights_2 @ values\n",
        "print(context_vector_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plQDfvsEqi81",
        "outputId": "0ad7a319-27a7-4e14-a2bd-149f113d3e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalizing the Process to Compute All Context Vectors\n",
        "\n",
        "So we've implemented weights, and we just computed the attention weights (softmax dot products) for the second word. This process can be optimized now. It's all there, in my head, some parts floating around not yet optimized yet. This will be an important chapter to code myself. Right now, the one new thing is '@' notation which replaces embedded for loops.\n",
        "\n",
        "d_in, d_out are confusing though"
      ],
      "metadata": {
        "id": "y7DWX-vMosO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "K &= X \\;@\\; W_{\\mathrm{key}},\\\\\n",
        "Q &= X \\;@\\; W_{\\mathrm{query}},\\\\\n",
        "V &= X \\;@\\; W_{\\mathrm{value}},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{scores} &= Q \\;@\\; K^{T},\\\\\n",
        "\\mathrm{weights} &= \\mathrm{softmax}\\!\\Bigl(\\tfrac{\\mathrm{scores}}{\\sqrt{d_{\\mathrm{key}}}}\\Bigr),\\\\\n",
        "\\mathrm{context} &= \\mathrm{weights} \\;@\\; V.\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "e5txc84d6hK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x): #x is an entire input tensor\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value #access the first by values[0], same for above\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n"
      ],
      "metadata": {
        "id": "nkh_7dcRoQvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "AVffbznlxHUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Casual Attention\n",
        "\n",
        "Realistic text geneartion relies mostly on words that come prior. As in, we won't have access to future words in a sentence. Let's first calculate the tensor for the attention weights of this given sentence."
      ],
      "metadata": {
        "id": "LtXGfkQM6_Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2\n",
        "\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)\n"
      ],
      "metadata": {
        "id": "tJAQA0c3xIDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4131125c-3fb6-4c48-8a65-bf11c8b9f0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
            "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
            "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
            "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
            "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now applying masks, making sure softmax is still applied and sums to one."
      ],
      "metadata": {
        "id": "W2rGRbUm-oMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "masked_simple = mask_simple * attn_weights\n",
        "\n",
        "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums #same\n",
        "\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "\n",
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1) #same\n",
        "print(attn_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2UsTkm19wjz",
        "outputId": "bf78ac67-abc5-436b-9b80-8d2d053b5d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
            "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropouts\n",
        "\n",
        "A technique used in deep learning. I'm noticing that these matrices have the same shape as the input tensor, and that theindividual context vecs have the same height and are stored. ==ok so dimensions might be a little more obvious==."
      ],
      "metadata": {
        "id": "13nyKhn2lNcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "print(dropout(attn_weights))\n"
      ],
      "metadata": {
        "id": "TPpj9Ezi-5xR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9a7687-6a19-42eb-d22b-4c8b40f1d0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0335, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4889, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3418, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Developing new SAC with Dropouts\n",
        "\n",
        "full class with functionality."
      ],
      "metadata": {
        "id": "aOywfLeMmQvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkQB55KjlRV3",
        "outputId": "fe79d09b-af7b-435d-865a-ba48b79e47e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]],\n",
            "\n",
            "        [[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Explanation for Attention with Batches\n",
        "\n",
        "$$\n",
        "X \\;=\\;\n",
        "\\begin{bmatrix}\n",
        "X^{(1)} \\\\[4pt]\n",
        "X^{(2)}\n",
        "\\end{bmatrix}\n",
        "\\;\\in\\;\\mathbb{R}^{2\\times6\\times3}\n",
        "$$\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "Q^{(b)} = X^{(b)}\\,W_Q,\\quad\n",
        "K^{(b)} = X^{(b)}\\,W_K,\\quad\n",
        "V^{(b)} = X^{(b)}\\,W_V\n",
        "\\;\\in\\;\\mathbb{R}^{6\\times3}.\n",
        "$$\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "$$\n",
        "\\omega^{(b)} \\;=\\; Q^{(b)}\\,\\bigl(K^{(b)}\\bigr)^{T}\n",
        "\\;\\in\\;\\mathbb{R}^{6\\times6}\n",
        "$$\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "$$\n",
        "\\widetilde \\alpha^{(b)}\n",
        "= \\text{masked}(\\alpha^{b}) \\rightarrow\n",
        "\\text{softmax}\\!\\Bigl(\\tfrac{1}{\\sqrt{D}}\\,\\widetilde \\alpha^{(b)}\\Bigr)\n",
        "\\;\\in\\;\\mathbb{R}^{6\\times6}.\n",
        "$$\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "$$\n",
        "z^{(b)}\n",
        "= A^{(b)}\\,V^{(b)}\n",
        "\\;\\in\\;\\mathbb{R}^{6\\times3},\n",
        "\\quad\n",
        "C^{(b)}_{i,:}\n",
        "= \\sum_{j=1}^{6} A^{(b)}_{ij}\\,V^{(b)}_{j,:}.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "\n",
        "$$\n",
        "z =\n",
        "\\begin{bmatrix}\n",
        "C^{(1)} \\\\[4pt]\n",
        "C^{(2)}\n",
        "\\end{bmatrix}\n",
        "\\;\\in\\;\\mathbb{R}^{2\\times6\\times3}.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "EUI8lhuZua0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Blurb about Masking\n",
        "\n",
        "We define a mask \\(M\\in\\mathbb{R}^{T\\times T}\\) that blocks future tokens:\n",
        "$$\n",
        "M_{ij} =\n",
        "\\begin{cases}\n",
        "0, & j \\le i,\\\\\n",
        "-\\infty, & j > i.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "% Applying the mask\n",
        "\\noindent\n",
        "Add \\(M\\) to the raw scores before softmax so that any “future” positions get zero weight:\n",
        "$$\n",
        "\\widetilde S^{(b)} = S^{(b)} + M,\n",
        "\\quad\n",
        "A^{(b)} = \\text{softmax}\\!\\bigl(\\tfrac{1}{\\sqrt{D}}\\,\\widetilde S^{(b)}\\bigr).\n",
        "$$"
      ],
      "metadata": {
        "id": "c8ldokt4xePC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # This is the mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n",
        "        # in the mask creation further below.\n",
        "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n",
        "        # do not exceed `context_length` before reaching this forward method.\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f7EnEv_mVMR",
        "outputId": "b2524410-ac1b-4e2f-e1df-7cf9375ebe5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "wO72RKUy0ttn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length,\n",
        "      dropout, num_heads, qkv_bias=False):\n",
        "\n",
        "      super().__init__()\n",
        "      self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
        "                             )\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "ATBgeFTmsVfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1] # This is the number of tokens, this is standard\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in, d_out, context_length, 0.0, num_heads=2\n",
        ")\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVQ9lz1E4Ch3",
        "outputId": "23808536-f25b-4e8f-af86-0bb2fc77e10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C72ebEs75Wcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refined MultiHead Class"
      ],
      "metadata": {
        "id": "J5PBwEUR5X5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n",
        "        # this will result in errors in the mask creation further below.\n",
        "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n",
        "        # do not exceed `context_length` before reaching this forwar\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n"
      ],
      "metadata": {
        "id": "ts7Ffpv74mTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}