{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y_Y9UJaV3AP"
      },
      "source": [
        "## What is attention mechanism\n",
        "\n",
        "*   Words represented as vectors don't consider context (bat (baseball), bat (animal) are the same vector\n",
        "*  Attention mechanisms changes the original vector with maybe multiple meanings by considering it's context. SUPA interesting.\n",
        "* Even more, when the weights are applied, a vector for 'bat'  (the animal) can be shifted into a direction of maybe caves, blindness, etc..\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzxMiHQJXgFl"
      },
      "source": [
        "### Omega -> Context Vector\n",
        "\n",
        "### Self-Attention Math Steps\n",
        "\n",
        "Given a sequence of token embeddings \\( x_1, x_2, \\dots, x_n \\), we compute:\n",
        "\n",
        "1. Assume weights would be calculated through training, seeing how close the prediction was to the next word.\n",
        "\n",
        "$$\n",
        "Q_i = x_i W^Q, \\quad K_j = x_j W^K, \\quad V_j = x_j W^V\n",
        "$$\n",
        "\n",
        "2. **Raw attention scores (Ï‰)** using dot product between Query and Key:\n",
        "\n",
        "$$\n",
        "\\omega_{ij} = Q_i \\cdot K_j^T\n",
        "$$\n",
        "\n",
        "3. **Normalize with softmax** to get attention weights:\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} = \\text{softmax}(\\omega_{ij})\n",
        "$$\n",
        "\n",
        "4. **Compute context vector \\( z_i \\)** (a weighted sum of Value vectors):\n",
        "\n",
        "$$\n",
        "z_i = \\sum_{j=1}^{n} \\alpha_{ij} V_j\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC2OYBcCTyhE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rYHoWa9bD1W"
      },
      "source": [
        "### Simple Calculation of Attention Weights\n",
        "\n",
        "Magnitude of  $\n",
        "\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos(\\theta)\n",
        "$ determines how aligned two vectors are, thus a mathematical representation of context when words are represented in space.  After we calculate the dot product of the query vector (the current vector we are at) with the vectors of every other word in the context, we create this score\n",
        "\n",
        "$$\n",
        "\\vec{\\alpha_{2j}} =\n",
        "\\begin{bmatrix}\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.9544 \\\\\n",
        "1.4950 \\\\\n",
        "1.4754 \\\\\n",
        "0.8434 \\\\\n",
        "0.7070 \\\\\n",
        "1.0865\n",
        "\\end{bmatrix} = \\sum_{j=1}^{n} \\alpha_{2j}\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS9rW5eZZBdi",
        "outputId": "63d7e02c-617e-40bb-e578-8fdf77528fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1]\n",
        "\n",
        "#start an empty vector for the attention score, with the specific size of the\n",
        "#sentence\n",
        "print(inputs.shape[0])\n",
        "attention_score_x2 = torch.empty(inputs.shape[0])\n",
        "for (index, x_i) in enumerate(inputs):\n",
        "  attention_score_x2[index] = torch.dot(x_i, query)\n",
        "\n",
        "print(attention_score_x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OHo7FCZe8-p"
      },
      "source": [
        "### Normalizing\n",
        "\n",
        "These sum to one through a normailization process that, for lack of better words, yields 'meaningful' results. Mathematically, for later implementation. But in general just use the pytorch one -- it's robust.\n",
        "\n",
        "$$$$\n",
        "\n",
        "$$\n",
        "\\vec{z_2} =\n",
        "\\begin{bmatrix}\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.9544 \\\\\n",
        "1.4950 \\\\\n",
        "1.4754 \\\\\n",
        "0.8434 \\\\\n",
        "0.7070 \\\\\n",
        "1.0865\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\text{softmax}(x_i)\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.1385 \\\\\n",
        "0.2379 \\\\\n",
        "0.2333 \\\\\n",
        "0.1240 \\\\\n",
        "0.1082 \\\\\n",
        "0.1581\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "$$$$\n",
        "\n",
        "where\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRqoiQ9dbXQI",
        "outputId": "be79ac02-5524-4721-d742-f91294bdaf8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refined Attention Weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def softmax_naive(x):\n",
        " return torch.exp(x) / torch.exp(x).sum(dim=0) #dim=0 because we're summing a column.\n",
        "\n",
        "\n",
        "attn_weights_2_solid = torch.softmax(attention_score_x2, dim=0)\n",
        "print(\"Refined Attention Weights:\", attn_weights_2_solid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlKfsVEgj95S"
      },
      "source": [
        "### Finalizing the Calculation of $z$\n",
        "$$\n",
        "\\vec{\\alpha_{2j}} =\n",
        "\\begin{bmatrix}\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.9544 \\\\\n",
        "1.4950 \\\\\n",
        "1.4754 \\\\\n",
        "0.8434 \\\\\n",
        "0.7070 \\\\\n",
        "1.0865\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\text{softmax}(x_i)\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.1385 \\\\\n",
        "0.2379 \\\\\n",
        "0.2333 \\\\\n",
        "0.1240 \\\\\n",
        "0.1082 \\\\\n",
        "0.1581\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\space\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\vec{z_{2}} =\n",
        "\\begin{bmatrix}\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing \\\\\n",
        "\\varnothing\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\alpha_{21} V_1 + \\alpha_{22} V_2 + \\cdots\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0.4371 \\\\\n",
        "0.4371 \\\\\n",
        "0.4371 \\\\\n",
        "0.4371 \\\\\n",
        "0.4371 \\\\\n",
        "0.4371\n",
        "\\end{bmatrix} = z_2 =\\sum_j \\alpha_{2j}V_j\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5czdQmfue-1k",
        "outputId": "e2db830d-9320-431f-f17b-6f21acd23cb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371])\n"
          ]
        }
      ],
      "source": [
        "context_vector = torch.empty(inputs.shape[0])\n",
        "\n",
        "for (index, x_j) in enumerate(query):\n",
        "  context_vector += attn_weights_2_solid[index] * x_j\n",
        "\n",
        "print(context_vector)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsQCSvQFjsnf"
      },
      "source": [
        "## Computing Attention Weights for all Input Tokens (3.3.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "embEVgvdkrQr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
