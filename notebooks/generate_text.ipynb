{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Defining some configurations for the model"
      ],
      "metadata": {
        "id": "msWC5hYNo3gl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi3Bu5A7nFKp"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        " \"vocab_size\": 50257, # Vocabulary size\n",
        " \"context_length\": 1024, # Context length\n",
        " \"emb_dim\": 768, # Embedding dimension\n",
        " \"n_heads\": 12, # Number of attention heads\n",
        " \"n_layers\": 12, # Number of layers\n",
        " \"drop_rate\": 0.1, # Dropout rate\n",
        " \"qkv_bias\": False # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Model Architecture"
      ],
      "metadata": {
        "id": "kR7BHJZZrCJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # Use a placeholder for TransformerBlock\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        # Use a placeholder for LayerNorm\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # A simple placeholder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This block does nothing and just returns its input.\n",
        "        return x\n",
        "\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # The parameters here are just to mimic the LayerNorm interface.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This layer does nothing and just returns its input.\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OfFfYSKKrj5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tok Embeddings / Pos Embeddings Notes\n",
        "\n",
        "% Definitions:\n",
        "% V = vocab size, d = emb dim, B = batch size, T = seq length, L = max context length\n",
        "% X ∈ {0,…,V-1}^{B×T}, E ∈ ℝ^{V×d}, P ∈ ℝ^{L×d}\n",
        "\n",
        "$$\n",
        "\\mathrm{tok\\_embeds}_{i,t}\n",
        "= E_{\\,X_{i,t},:\\,}\n",
        "\\in \\mathbb{R}^{V \\times d},\n",
        "\\quad\n",
        "\\mathrm{tok\\_embeds}\\in\\mathbb{R}^{B\\times T\\times d}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathrm{pos\\_embeds}_{i,t}\n",
        "= P_{\\,t,:\\,}\n",
        "\\in \\mathbb{R}^{L \\times D},\n",
        "\\quad\n",
        "\\mathrm{pos\\_embeds}\\in\\mathbb{R}^{B\\times T\\times d}\n",
        "$$\n",
        "\n",
        "$$\n",
        "X^{(0)}_{i,t}\n",
        "= \\mathrm{tok\\_embeds}_{i,t}\n",
        "+ \\mathrm{pos\\_embeds}_{i,t}\n",
        "\\in \\mathbb{R}^d\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\widetilde X^{(0)}_{i,t}\n",
        "= \\mathrm{Dropout}\\bigl(X^{(0)}_{i,t}\\bigr)\n",
        "\\in \\mathbb{R}^d,\n",
        "\\quad\n",
        "\\widetilde X^{(0)}\\in\\mathbb{R}^{B\\times T\\times d}\n",
        "$$\n",
        "\n",
        "\n",
        "## Notes on Calls to Embed Functions\n",
        "\n",
        "$$\n",
        "\\mathrm{tok\\_embeds}\n",
        "= \\mathbf{E}\\bigl[\\mathrm{in\\_idx}\\bigr]\n",
        "\\quad\\Longrightarrow\\quad\n",
        "\\bigl(\\mathrm{tok\\_embeds}\\bigr)_{i,t,k}\n",
        "= E_{\\,\\mathrm{in\\_idx}_{i,t}\\,,\\,k}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "&\\in \\mathbb{R},\\\\\n",
        "&\\mathrm{shape:}\\;(B \\times T \\times d)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "% 2) Positional lookup:\n",
        "$$\n",
        "\\mathrm{pos\\_indices}\n",
        "= \\bigl[0,1,\\dots,T-1\\bigr],\n",
        "\\quad\n",
        "\\mathrm{pos\\_embeds}\n",
        "= \\mathbf{P}\\bigl[\\mathrm{pos\\_indices}\\bigr]\n",
        "\\quad\\Longrightarrow\\quad\n",
        "\\bigl(\\mathrm{pos\\_embeds}\\bigr)_{i,t,k}\n",
        "= P_{\\,t\\,,\\,k}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "&\\in \\mathbb{R},\\\\\n",
        "&\\mathrm{shape:}\\;(B \\times T \\times d)\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "XC948dYAxREy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes on Process up to Logis\n",
        "\n",
        "```python\n",
        "x = tok_embeds + pos_embeds    # shape: [B, L, D]\n",
        "x = self.drop_emb(x)           # still [B, L, D]\n",
        "x = self.trf_blocks(x)         # [B, L, D] after every block preserves D\n",
        "x = self.final_norm(x)         # [B, L, D]\n",
        "```\n",
        "\n",
        "at this point we have $ X \\in \\mathbb{R}^{(BS \\times SL \\times ED)} $\n",
        "\n",
        "At the logits step, we have  $ \\text{logits} \\in \\mathbb{R}^{(BS \\times SL \\times VS)} $"
      ],
      "metadata": {
        "id": "RLELOUgH9l4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Text"
      ],
      "metadata": {
        "id": "m-OVgDmI02QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YaxRLpjR1Sd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUo6wy0byzLX",
        "outputId": "2e507183-a8a2-47ac-d26f-703ba3b3b155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(\"Output shape:\", logits.shape)\n",
        "print(logits, logits.shape)"
      ],
      "metadata": {
        "id": "8jkS1H2B03yt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29d6782-d636-4bb6-8526-39e93b53e89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>) torch.Size([2, 4, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5)\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO7MOQZx2lAa",
        "outputId": "5eb53d32-e90c-44cc-f8c1-c6260ad1de92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PPuiXf-UBV0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}